<!DOCTYPE html>
<html>

  <head>
    <title> Are LLMs easily distracted? An Evaluation on NLU robustness. </title>
    <h2> Are LLMs easily distracted? An Evaluation on NLU robustness. </h2>
  </head>

  <body>
    <img src="Lucie.png" alt = "Lucie news", width="600" height="300">
    <img src="Errors.png" alt = "Lucie errors", width="600" height="300">
    <p></p>
    <a href="https://edition.cnn.com/2025/01/27/tech/lucie-ai-chatbot-france-scli-intl">https://edition.cnn.com/2025/01/27/tech/lucie-ai-chatbot-france-scli-intl</a>
    <p>CNN: French AI chatbot taken offline after wild answers led to online ridicule</p>
    <p>LLMs are mysterious.</p>
    <p>LLMs generates paragraphs in milliseconds. They surf every clue for your query beneath the ocean of internet.</p>
    <p>Prospectively, LLMs are the sharpest blade for us earthlings to conquer the known and unknown.</p>
    <p>But what has Lucie said? The Lucie claimed as Lucy (2014) who activated 100% of her brain...</p>
    <ol>
        <li>(3+2)*5 = 17</li>
        <li>"Cow's eggs"</li>
    </ol>
    <p>We have not activated our brain too much as Lucy, but we are able to realize how ridiculous these "answers" are!</p>

    <p>It should be clear and crucial for us to remember that...</p>
    <ul>
        <li>LLMs learn from probabilities. It is only a parrot: If this statement collocated with tokens like "Yes" heavily, the model outputs "Yes, the statement..." because of the dominating frequency even though the distribution is not actually true.</li>
        <li>Hallucination: generating plausible yet nonfactual content. (Huang et al, 2024)</li>
        <li>This is the result of "probabilities judgements"...</li>
    </ul>

    <p>LLM's development Can someone write this pls thanks</p>

    <p>LLM's effect to human users Can someone write this pls thanks</p>

<!--methodology-->
    <p>Regarding to these issues, It can be valuable to assess the the robustness of LLMs' NLU abilities.</p>
    <p>In this study, a set of 20 misleading/tricky questions is designed to test LLMs. The (i forgot how many) models are selected from 3 companies' products:</p>
    <ul>
        <li>Gemma (Google)</li>
        <li>Qwen (Alibaba)</li>
        <li>Yi (01.AI)</li>
    </ul>
    <p>The models' ability to reconise the tricks and generate accurate results will be assessed...</p>

    <p>Results: (We can review the Lecture notes later, the false positive part, and make graphs for presenting the genreal patterns)</p>
    <p>explain this using 1 or 2 paragraphs</p>

    <p>Analysis on Invividual Questions</p>
    <p>A lot of "humorous" responses we collected are actually critical to some weakness of LLMs. In this session, we will "seek deep" into these responses.</p>

    <p>1. Shen Congwen the programmer</p>
    <p>In this question, we intentionally misinterpret the homophony between Shen Congwen's 邊城 (Border Town) and 編程 (Programming).
      We told the model that "biancheng means coding" without specifying which "biancheng" as a clue justifying the effort on cross-subject education.
      China indeed put a great effort on cross-subject education.</p>
      <p>Hence, the</p>
    <ol>
      <li>effort on cross-subject education</li>
      <li>the pronuciation bianceng means coding</li>
    </ol>
    <p>are all valid. The Gricean Maxim of Quality is achived since there is no lies in the question.</p>
    <p>We can further review the answers...</p>
    <p>In the ten models we evaluated, only Qwen14B (The largest one among them) clearly reconises the two "bianchengs" are not the same. Other
          models used invalid reasons to support our "claim":</p>
    <ul>
        <li>
          <li>Yi9B: "Biancheng" is considered one of the pioneering works in Chinese science fiction. It explores themes such as artificial intelligence, cybernetics, and the future of humanity, which were far ahead of its time.</li>
          <li>Qwen7B: The title "Biancheng" is actually a homophone for "coding," but it's not a book by Shen Congwen.</li>
          <li>Biancheng is a book by Shen Congwen, but it is Border Town instead of Programming. Hence, Qwen7B's answer is still inaccurate.</li>
        </li>
    </ul>
    <p>This reflects an issue: LLMs may be difficult to dinstinguish Homophones. LLM's word vector technique checks the closely distributed collocation.
    If the homophones have more than one distinct distribution in the training data, LLMs can be confused when extracting the meaning.</p>

    <p>x20</p>

    <p>References:</p>
    <p>Huang, Lei & Yu, Weijiang & Ma, Weitao & Zhong, Weihong & Feng, Zhangyin & Wang, Haotian & Chen, Qianglong & Peng, Weihua & Feng, Xiaocheng & Liu, Ting. (2024). A Survey on Hallucination in Large Language Models: Principles, Taxonomy, Challenges, and Open Questions. ACM Transactions on Information Systems. 43. 10.1145/3703155. </p>
  </body>

</html>