<!DOCTYPE html>
<html>

  <head>
    <title> Are LLMs easily distracted? An Evaluation on NLU robustness. </title>
    <h2> Are LLMs easily distracted? An Evaluation on NLU robustness. </h2>
  </head>

  <body>
    <img src="Lucie.png" alt = "Lucie.png", width="600" height="300">
    <img src="Errors.png" alt = "Lucie.png", width="600" height="300">
    <p></p>
    <a href="https://edition.cnn.com/2025/01/27/tech/lucie-ai-chatbot-france-scli-intl">https://edition.cnn.com/2025/01/27/tech/lucie-ai-chatbot-france-scli-intl</a>
    <p>CNN: French AI chatbot taken offline after wild answers led to online ridicule</p>
    <p>LLMs are mysterious.</p>
    <p>LLMs generates paragraphs in milliseconds. They surf every clue for your query beneath the ocean of internet.</p>
    <p>Prospectively, LLMs are the sharpest blade for us earthlings to conquer the known and unknown.</p>
    <p>But what has Lucie said? The Lucie claimed as Lucy (2014) who activated 100% of her brain...</p>
    <ol>
        <li>(3+2)*5 = 17</li>
        <li>"Cow's eggs"</li>
    </ol>
    <p>We have not activated our brain too much as Lucy, but we are able to realize how ridiculous these "answers" are!</p>

    <p>It should be clear and crucial for us to remember that...</p>
    <ul>
        <li>LLMs learn from probabilities. It is only a parrot: If this statement collocated with tokens like "Yes" heavily, the model outputs "Yes, the statement" because of the dominating frequency even though the distribution is not actually true.</li>
        <li>Hallucination: generating plausible yet nonfactual content. (Huang et al, 2024)</li>
        <li>This is the result of "probabilities judgements"...</li>
    </ul>

    <p></p>
  </body>

</html>